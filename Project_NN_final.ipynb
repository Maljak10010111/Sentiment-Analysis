{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "094fa655",
      "metadata": {
        "id": "094fa655"
      },
      "source": [
        "# Word embedding and RNN for sentiment analysis\n",
        "\n",
        "The goal of the following notebook is to predict whether a written\n",
        "critic about a movie is positive or negative. For that we will try\n",
        "three models. A simple linear model on the word embeddings and\n",
        "recurrent neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7fd09a88",
      "metadata": {
        "id": "7fd09a88"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable, List\n",
        "import appdirs                  # Used to cache pretrained embeddings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim import Adam, Optimizer\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext import datasets\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4WAu67ou6WXd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WAu67ou6WXd",
        "outputId": "1335674e-75ac-47eb-88dd-ec6d53331639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b98d925",
      "metadata": {
        "id": "9b98d925"
      },
      "source": [
        "## The IMDB dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f5ab1832",
      "metadata": {
        "id": "f5ab1832"
      },
      "outputs": [],
      "source": [
        "torch_cache = appdirs.user_cache_dir('pytorch')\n",
        "train_iter, test_iter = datasets.IMDB(root=torch_cache, split=('train', 'test'))\n",
        "\n",
        "TRAIN_SET = list(train_iter)\n",
        "TEST_SET = list(test_iter)\n",
        "random.shuffle(TRAIN_SET)\n",
        "random.shuffle(TEST_SET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "522a5228",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "522a5228",
        "outputId": "a40e60de-b502-417e-8a6d-f78759532a94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2,\n",
              " \"This is such a great movie to watch with young children. I'm always looking for an excuse to watch it over & over. Gena was good, Cheech was fun,the Russian was good, Maria was adorable & of course Paulie was the best!\")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "TRAIN_SET[5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "386bd9be",
      "metadata": {
        "id": "386bd9be"
      },
      "source": [
        "## Global variables\n",
        "\n",
        "First let's define a few variables. `EMBEDDING_DIM` is the dimension\n",
        "of the vector space used to embed all the words of the vocabulary.\n",
        "`SEQ_LENGTH` is the maximum length of a sequence, `BATCH_SIZE` is\n",
        "the size of the batches used in stochastic optimization algorithms\n",
        "and `NUM_EPOCHS` the number of times we are going thought the entire\n",
        "training set during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3d571ecc",
      "metadata": {
        "id": "3d571ecc"
      },
      "outputs": [],
      "source": [
        "SEQ_LENGTH = 64\n",
        "BATCH_SIZE = 512\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7907d78d",
      "metadata": {
        "id": "7907d78d"
      },
      "source": [
        "We first need a tokenizer that take a text a returns a list of\n",
        "tokens. There are many tokenizers available from other libraries.\n",
        "Here we use the one that comes with Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b8a57107",
      "metadata": {
        "id": "b8a57107",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffd03d1",
      "metadata": {
        "id": "0ffd03d1"
      },
      "source": [
        "## Building the vocabulary\n",
        "\n",
        "Then we need to define the set of words that will be understood by\n",
        "the model: this is the vocabulary. We build it from the training\n",
        "set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bad8f472",
      "metadata": {
        "id": "bad8f472"
      },
      "outputs": [],
      "source": [
        "# Costruzione del vocabolario\n",
        "def token_generation(data_iter: Iterable) -> List[str]:\n",
        "    for data_sample in data_iter:\n",
        "        yield tokenizer(data_sample[1])\n",
        "\n",
        "\n",
        "special_tokens = [\"<unk>\", \"<pad>\"]\n",
        "vocab = build_vocab_from_iterator(\n",
        "    token_generation(TRAIN_SET),\n",
        "    min_freq=10,\n",
        "    specials=special_tokens,\n",
        "    special_first=True)\n",
        "UNK_IDX, PAD_IDX = vocab.lookup_indices(special_tokens)\n",
        "VOCAB_SIZE = len(vocab)\n",
        "vocab.set_default_index(UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21d3890",
      "metadata": {
        "id": "f21d3890"
      },
      "source": [
        "\n",
        "To limit the number of tokens in the vocabulary, we specified\n",
        "`min_freq=10`: a token should be seen at least 10 times to be part\n",
        "of the vocabulary. Consequently some words in the training set (and\n",
        "in the test set) are not present in the vocabulary. We then need to\n",
        "set a default index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e4a2a74",
      "metadata": {
        "id": "9e4a2a74",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "# vocab['pouet']                  # Error\n",
        "                          # le parole sconosciute avranno indice 0 nel vocabolario\n",
        "#vocab['vdfbdfbdfbdf']\n",
        "#vocab['I']\n",
        "#vocab['am']\n",
        "#vocab['groot']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b44e13f",
      "metadata": {
        "id": "2b44e13f"
      },
      "source": [
        "# Collate function\n",
        "\n",
        "The collate function maps raw samples coming from the dataset to\n",
        "padded tensors of numericalized tokens ready to be fed to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d7db28dc",
      "metadata": {
        "id": "d7db28dc"
      },
      "outputs": [],
      "source": [
        "def text_to_tensor_fn(batch: List):\n",
        "    def text_to_tensor(text):\n",
        "        tokens = tokenizer(text)[:SEQ_LENGTH]\n",
        "        return torch.LongTensor(vocab(tokens))\n",
        "\n",
        "    src_batch = [text_to_tensor(text) for _, text in batch]\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = torch.Tensor([label - 1 for label, _ in batch])\n",
        "    return src_batch, tgt_batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "39e88720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39e88720",
        "outputId": "054eb892-7512-4d99-fe5a-0a39c83093a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 13],\n",
              "         [246],\n",
              "         [  0]]),\n",
              " tensor([0.]))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_to_tensor_fn([\n",
        "    (1, \"i am Groot\")\n",
        "])\n",
        "\n",
        "# tensor([[ 13], [246], [ 0]]): Questo rappresenta il tensore risultante dalla tokenizzazione e dal padding delle sequenze. Ogni numero intero all'interno del tensore corrisponde a un indice nel vocabolario.\n",
        "# tensor([0.]): Questo rappresenta l'etichetta associata all'elemento del batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "679a6fcf",
      "metadata": {
        "id": "679a6fcf",
        "lines_to_next_cell": 2
      },
      "source": [
        "## Training a linear classifier with an embedding\n",
        "\n",
        "We first test a simple linear classifier on the word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "398e8282",
      "metadata": {
        "id": "398e8282",
        "lines_to_next_cell": 2
      },
      "source": [
        "We need to implement an accuracy function to be used in the `Trainer`\n",
        "class (see below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5b8b4cf7",
      "metadata": {
        "id": "5b8b4cf7"
      },
      "outputs": [],
      "source": [
        "def accuracy(predictions, labels):\n",
        "    return torch.sum((torch.sigmoid(predictions) > 0.5).float() == (labels > .5)).item() / len(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb02a80",
      "metadata": {
        "id": "cbb02a80"
      },
      "source": [
        "Train and test functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d43387df",
      "metadata": {
        "id": "d43387df",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def train_epoch(model: nn.Module, optimizer: Optimizer):\n",
        "    model.train()\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "    train_dataloader = DataLoader(TRAIN_SET, batch_size=BATCH_SIZE, collate_fn=text_to_tensor_fn)\n",
        "\n",
        "    matches = 0\n",
        "    for sequences, labels in train_dataloader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(sequences)\n",
        "        loss = loss_fn(predictions, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = accuracy(predictions, labels)\n",
        "        matches += len(predictions) * acc\n",
        "\n",
        "    return matches / len(TRAIN_SET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c84b4b6d",
      "metadata": {
        "id": "c84b4b6d"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module):\n",
        "    model.eval()\n",
        "    val_dataloader = DataLoader(TEST_SET, batch_size=BATCH_SIZE, collate_fn=text_to_tensor_fn)\n",
        "\n",
        "    matches = 0\n",
        "    for sequences, labels in val_dataloader:\n",
        "\n",
        "        predictions = model(sequences)\n",
        "        acc = accuracy(predictions, labels)\n",
        "        matches += len(predictions) * acc\n",
        "\n",
        "    return matches / len(TEST_SET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "496136a3",
      "metadata": {
        "id": "496136a3",
        "lines_to_next_cell": 1
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer):\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        train_acc = train_epoch(model, optimizer)\n",
        "        val_acc = evaluate(model)\n",
        "        print(\n",
        "            f\"Epoch: {epoch}, \"\n",
        "            f\"Train acc: {train_acc:.3f}, \"\n",
        "            f\"Val acc: {val_acc:.3f} \"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0723838a",
      "metadata": {
        "id": "0723838a"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(model, sentence):\n",
        "    \"Predict sentiment of given sentence according to model\"\n",
        "\n",
        "    tensor, _ = text_to_tensor_fn([(0, sentence)])\n",
        "    prediction = model(tensor)\n",
        "    pred = torch.sigmoid(prediction)\n",
        "    return pred.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "65e07e5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e07e5f",
        "lines_to_next_cell": 2,
        "outputId": "372cb1f8-bb46-4c8f-9222-779cac565da8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/pytorch/glove.6B.zip: 862MB [02:41, 5.35MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:19<00:00, 20241.68it/s]\n"
          ]
        }
      ],
      "source": [
        "# # Load a GloVe pretrained embedding instead\n",
        "# GloVe --> Global Vectors for Word Representation\n",
        "# Download GloVe word embedding\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=\"100\", cache=torch_cache)\n",
        "vocab_vectors = glove.get_vecs_by_tokens(vocab.get_itos())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "48e1579f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48e1579f",
        "outputId": "7b1c53f7-7686-44ed-a85a-0b664cebf57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train acc: 0.498, Val acc: 0.502 \n",
            "Epoch: 2, Train acc: 0.570, Val acc: 0.506 \n",
            "Epoch: 3, Train acc: 0.597, Val acc: 0.504 \n",
            "Epoch: 4, Train acc: 0.612, Val acc: 0.504 \n",
            "Epoch: 5, Train acc: 0.625, Val acc: 0.506 \n",
            "Epoch: 6, Train acc: 0.634, Val acc: 0.505 \n",
            "Epoch: 7, Train acc: 0.640, Val acc: 0.507 \n",
            "Epoch: 8, Train acc: 0.645, Val acc: 0.505 \n",
            "Epoch: 9, Train acc: 0.646, Val acc: 0.503 \n",
            "Epoch: 10, Train acc: 0.650, Val acc: 0.504 \n"
          ]
        }
      ],
      "source": [
        "class GloVeEmbeddingNet(nn.Module):\n",
        "    def __init__(self, seq_length, vocab_vectors, freeze=True):\n",
        "        super().__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.embedding_dim = vocab_vectors.size(1)\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
        "        self.l1 = nn.Linear(self.seq_length * self.embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        flatten = embedded.view(-1, self.seq_length * self.embedding_dim)\n",
        "        return self.l1(flatten).squeeze()\n",
        "\n",
        "glove_embedding_net1 = GloVeEmbeddingNet(SEQ_LENGTH, vocab_vectors, freeze=True)\n",
        "optimizer = Adam(glove_embedding_net1.parameters())\n",
        "train(glove_embedding_net1, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4012315",
      "metadata": {
        "id": "a4012315"
      },
      "source": [
        "## Recurrent neural network with frozen pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e9a138ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9a138ba",
        "lines_to_next_cell": 1,
        "outputId": "9811576e-ae67-4919-c6f8-d07f9dd137b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Train acc: 0.571, Val acc: 0.666 \n",
            "Epoch: 2, Train acc: 0.706, Val acc: 0.706 \n",
            "Epoch: 3, Train acc: 0.729, Val acc: 0.731 \n",
            "Epoch: 4, Train acc: 0.745, Val acc: 0.744 \n",
            "Epoch: 5, Train acc: 0.760, Val acc: 0.756 \n",
            "Epoch: 6, Train acc: 0.772, Val acc: 0.763 \n",
            "Epoch: 7, Train acc: 0.778, Val acc: 0.770 \n",
            "Epoch: 8, Train acc: 0.785, Val acc: 0.772 \n",
            "Epoch: 9, Train acc: 0.792, Val acc: 0.773 \n",
            "Epoch: 10, Train acc: 0.796, Val acc: 0.774 \n"
          ]
        }
      ],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, vocab_vectors, freeze=True):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(vocab_vectors, freeze=freeze)\n",
        "        self.embedding_size = self.embedding.embedding_dim\n",
        "        self.input_size = self.embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size=self.input_size, hidden_size=self.hidden_size)\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x, h0=None):\n",
        "        if h0 is None:\n",
        "            batch_size = x.size(1)\n",
        "            h0 = torch.zeros(self.gru.num_layers, batch_size, self.hidden_size)\n",
        "\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.gru(embedded, h0)\n",
        "        return self.linear(hidden).squeeze()\n",
        "\n",
        "\n",
        "rnn = RNN(hidden_size=100, vocab_vectors=vocab_vectors)\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnn.parameters()), lr=0.001)\n",
        "train(rnn, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24dfd609",
      "metadata": {
        "id": "24dfd609"
      },
      "source": [
        "## Test function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "uFUJIS8QJpbs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFUJIS8QJpbs",
        "outputId": "ccb900d6-4218-4c4b-ddb6-8234f817530b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN1 Model Prediction: 0.9861965775489807 \n",
            "RNN2 Model Prediction: 0.1494850218296051 \n",
            "Glove1 Model Prediction: 0.6458622813224792 \n",
            "Glove2 Model Prediction: 0.3467015326023102 \n"
          ]
        }
      ],
      "source": [
        "sentence_bad = \"Whispers of Destiny' disappoints on multiple levels. The plot feels convoluted and overstretched, leaving audiences confused rather than engaged. Despite a talented cast, the characters lack depth, making it hard to invest in their journey. The CGI, particularly in crucial scenes, falls short, distracting from any potential emotional impact. The film's potential is overshadowed by poor execution, resulting in a forgettable and underwhelming cinematic experience.\"\n",
        "sentence_good = \"Intriguing from start to finish, 'Whispers of Destiny' captivates with its brilliant storytelling and exceptional performances. The cinematography beautifully enhances the magical world, while the soundtrack complements the emotional depth. Each twist keeps you on the edge, and the climax is both satisfying and surprising. A true cinematic gem that lingers in your thoughts, leaving a lasting impression.\"\n",
        "prediction_rnn1 = predict_sentiment(rnn, sentence_good)\n",
        "prediction_rnn2 = predict_sentiment(rnn, sentence_bad)\n",
        "prediction_glove1 = predict_sentiment(glove_embedding_net1, sentence_good)\n",
        "prediction_glove2 = predict_sentiment(glove_embedding_net1, sentence_bad)\n",
        "print(\n",
        "            f\"RNN1 Model Prediction: {prediction_rnn1} \\n\"\n",
        "            f\"RNN2 Model Prediction: {prediction_rnn2} \\n\"\n",
        "            f\"Glove1 Model Prediction: {prediction_glove1} \\n\"\n",
        "            f\"Glove2 Model Prediction: {prediction_glove2} \"\n",
        "      )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
